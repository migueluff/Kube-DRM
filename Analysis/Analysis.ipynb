{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a0ca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import os\n",
    "\n",
    "FILES_TO_PROCESS = {\n",
    "    'data/results_nas_profiling_20250815_000950.csv': ['NAS-BT'],\n",
    "    'data/results_hpc_profiling_20250815_105543.csv': ['SIM-pulse_spikes'] \n",
    "}\n",
    "\n",
    "def get_display_name(original_name):\n",
    "    name = original_name.lower()\n",
    "    name_mapping = {\n",
    "        'nas-bt': 'NAS-BT',\n",
    "        'nas-cg': 'NAS-CG', \n",
    "        'nas-ep': 'NAS-EP',\n",
    "        'nas-mg': 'NAS-MG',\n",
    "        'nas-sp': 'NAS-SP',\n",
    "        'hpc-pulse-spikes': 'SIM-pulse_spikes',\n",
    "        'hpc-sawtooth': 'SIM-sawtooth',\n",
    "        'hpc-staircase': 'SIM-staircase',\n",
    "        'hpc-epochs': 'SIM-epochs'\n",
    "    }\n",
    "    \n",
    "    for key, value in name_mapping.items():\n",
    "        if key in name:\n",
    "            return value\n",
    "    return original_name\n",
    "\n",
    "def load_and_filter_data():\n",
    "    all_data = []\n",
    "    \n",
    "    for file_path, apps_to_keep in FILES_TO_PROCESS.items():\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            df.columns = df.columns.str.strip()\n",
    "            df['Display Name'] = df['Job Name'].apply(get_display_name)\n",
    "            df_filtered = df[df['Display Name'].isin(apps_to_keep)].copy()\n",
    "            \n",
    "            if not df_filtered.empty:\n",
    "                all_data.append(df_filtered)\n",
    "                \n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "    \n",
    "    return pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()\n",
    "\n",
    "def preprocess_data(df):\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce')\n",
    "    \n",
    "    numeric_cols = ['Pod CPU Usage (m)', 'Pod Memory Usage (Mi)', 'VPA Target CPU (m)', 'VPA Target Mem (Mi)']\n",
    "    for col in numeric_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    if not df['VPA Target Mem (Mi)'].empty and df['VPA Target Mem (Mi)'].max() > (1024**2):\n",
    "        df['VPA Target Mem (Mi)'] = df['VPA Target Mem (Mi)'] / (1024**2)\n",
    "    \n",
    "    df['VPA Target CPU (m)'].fillna(0, inplace=True)\n",
    "    df['VPA Target Mem (Mi)'].fillna(0, inplace=True)\n",
    "    df.dropna(subset=['Timestamp', 'Display Name', 'Pod CPU Usage (m)', 'Pod Memory Usage (Mi)'], inplace=True)\n",
    "    \n",
    "    df.sort_values(by=['Display Name', 'Timestamp'], inplace=True)\n",
    "    df['Start Time'] = df.groupby('Display Name')['Timestamp'].transform('min')\n",
    "    df['Elapsed Time (s)'] = (df['Timestamp'] - df['Start Time']).dt.total_seconds()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_charts(df, cpu_filename='cpu_analysis.pdf', mem_filename='memory_analysis.pdf'):\n",
    "    if df.empty:\n",
    "        return\n",
    "    \n",
    "    app_names = sorted(df['Display Name'].unique())\n",
    "    num_apps = len(app_names)\n",
    "    \n",
    "    if num_apps == 0:\n",
    "        return\n",
    "\n",
    "    plt.rcParams['font.size'] = 16\n",
    "    plt.rcParams['axes.linewidth'] = 0.8\n",
    "    \n",
    "    with PdfPages(cpu_filename) as pdf:\n",
    "        fig_cpu, axes_cpu = plt.subplots(num_apps, 1, figsize=(8, 4 * num_apps), squeeze=False)\n",
    "        \n",
    "        for i, app_name in enumerate(app_names):\n",
    "            ax = axes_cpu[i, 0]\n",
    "            app_data = df[df['Display Name'] == app_name]\n",
    "    \n",
    "            ax.fill_between(app_data['Elapsed Time (s)'], app_data['Pod CPU Usage (m)'], \n",
    "                           color='#2E86AB', alpha=0.7, label='CPU Usage')\n",
    "            ax.plot(app_data['Elapsed Time (s)'], app_data['VPA Target CPU (m)'], \n",
    "                   color='#F24236', linestyle='--', linewidth=2, label='VPA Recommendation')\n",
    "            \n",
    "            #ax.set_title(f'{app_name} - CPU Usage', fontweight='bold', pad=15)\n",
    "            ax.set_xlabel('Elapsed Time (s)', fontweight='bold')\n",
    "            ax.set_ylabel('CPU (mCores)', fontweight='bold')\n",
    "            ax.legend(frameon=True, fancybox=True, shadow=True)\n",
    "            ax.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "            ax.set_ylim(bottom=0)\n",
    "            ax.set_xlim(left=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        pdf.savefig(fig_cpu, bbox_inches='tight', dpi=300)\n",
    "        plt.close(fig_cpu)\n",
    "\n",
    "    with PdfPages(mem_filename) as pdf:\n",
    "        fig_mem, axes_mem = plt.subplots(num_apps, 1, figsize=(8, 4 * num_apps), squeeze=False)\n",
    "        \n",
    "        for i, app_name in enumerate(app_names):\n",
    "            ax = axes_mem[i, 0]\n",
    "            app_data = df[df['Display Name'] == app_name]\n",
    "            \n",
    "            ax.fill_between(app_data['Elapsed Time (s)'], app_data['Pod Memory Usage (Mi)'], \n",
    "                           color='#A23B72', alpha=0.7, label='Memory Usage')\n",
    "            ax.plot(app_data['Elapsed Time (s)'], app_data['VPA Target Mem (Mi)'], \n",
    "                   color='#F18F01', linestyle='--', linewidth=2, label='VPA Recommendation')\n",
    "            \n",
    "            #ax.set_title(f'{app_name} - Memory Usage', fontweight='bold', pad=15)\n",
    "            ax.set_xlabel('Elapsed Time (s)', fontweight='bold')\n",
    "            ax.set_ylabel('Memory (MiB)', fontweight='bold')\n",
    "            ax.legend(frameon=True, fancybox=True, shadow=True)\n",
    "            ax.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "            ax.set_ylim(bottom=0)\n",
    "            ax.set_xlim(left=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        pdf.savefig(fig_mem, bbox_inches='tight', dpi=300)\n",
    "        plt.close(fig_mem)\n",
    "\n",
    "def main():\n",
    "    raw_data = load_and_filter_data()\n",
    "    processed_data = preprocess_data(raw_data)\n",
    "    \n",
    "    if not processed_data.empty:\n",
    "        create_charts(\n",
    "            processed_data, \n",
    "            cpu_filename='vpa_cpu.pdf', \n",
    "            mem_filename='vpa_memory.pdf'\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dc8a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success analysis chart saved to 'success_analysis.pdf'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "APP_DISPLAY_NAMES = [\n",
    "    'NAS-BT', 'NAS-CG', 'NAS-EP', 'NAS-MG', 'NAS-SP',\n",
    "    'SIM-pulse_spikes', 'SIM-sawtooth', 'SIM-staircase', 'SIM-epochs'\n",
    "]\n",
    "\n",
    "SCENARIOS_TO_PROCESS = {\n",
    "    'Guaranteed': 'data/results_combined_guaranteed_20250815_153205.csv',\n",
    "    'Extreme': 'data/results_combined_extreme_20250815_144056.csv',\n",
    "    'Clairvoyant': 'data/results_combined_clairvoyant_20250818_101541.csv'\n",
    "}\n",
    "\n",
    "def get_display_name(original_name):\n",
    "    \"\"\"Standardize job names for consistent filtering\"\"\"\n",
    "    name = original_name.lower()\n",
    "    name_mapping = {\n",
    "        'nas-bt': 'NAS-BT',\n",
    "        'nas-cg': 'NAS-CG', \n",
    "        'nas-ep': 'NAS-EP',\n",
    "        'nas-mg': 'NAS-MG',\n",
    "        'nas-sp': 'NAS-SP',\n",
    "        'hpc-pulse-spikes': 'SIM-pulse_spikes',\n",
    "        'hpc-sawtooth': 'SIM-sawtooth',\n",
    "        'hpc-staircase': 'SIM-staircase',\n",
    "        'hpc-epochs': 'SIM-epochs'\n",
    "    }\n",
    "    \n",
    "    for key, value in name_mapping.items():\n",
    "        if key in name:\n",
    "            return value\n",
    "    return original_name\n",
    "\n",
    "\n",
    "def find_successful_apps(file_path, scenario_name):\n",
    "    \"\"\"Find applications that completed successfully in a given scenario\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        successful_jobs = df[df['Pod Status'] == 'Succeeded']['Job Name'].unique()\n",
    "        return pd.DataFrame({'Job_Name': successful_jobs, 'Scenario': scenario_name})\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "def process_all_scenarios():\n",
    "    \"\"\"Process all scenario files and combine results\"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for scenario, file_path in SCENARIOS_TO_PROCESS.items():\n",
    "        result_df = find_successful_apps(file_path, scenario)\n",
    "        if result_df is not None:\n",
    "            all_results.append(result_df)\n",
    "    \n",
    "    if not all_results:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "\n",
    "    combined_df = pd.concat(all_results, ignore_index=True)\n",
    "    combined_df['Job_Name'] = combined_df['Job_Name'].apply(get_display_name)\n",
    "    combined_df = combined_df.drop_duplicates(subset=['Scenario', 'Job_Name'])\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def create_success_chart(df, output_filename='success_analysis.pdf'):\n",
    "    \"\"\"Create chart showing successful applications by scenario\"\"\"\n",
    "    if df.empty:\n",
    "        return \"No data available for visualization\"\n",
    "    \n",
    "\n",
    "    total_submitted_apps = len(APP_DISPLAY_NAMES)\n",
    "    counts_per_scenario = df.groupby('Scenario')['Job_Name'].nunique()\n",
    "    scenario_order = ['Guaranteed', 'Clairvoyant', 'Extreme']\n",
    "    counts_per_scenario = counts_per_scenario.reindex(scenario_order)\n",
    "    \n",
    "\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.rcParams['font.size'] = 16\n",
    "    \n",
    "    with PdfPages(output_filename) as pdf:\n",
    "        fig, ax = plt.subplots(figsize=(10, 7))\n",
    "        \n",
    "\n",
    "        scenario_names = counts_per_scenario.index.tolist()\n",
    "        executed_counts = counts_per_scenario.values.tolist()\n",
    "        colors = sns.color_palette('Blues', n_colors=len(scenario_names))\n",
    "        \n",
    "        bars = ax.bar(scenario_names, executed_counts, color=colors, \n",
    "                     edgecolor='black', width=0.7)\n",
    "        \n",
    "\n",
    "        ax.axhline(y=total_submitted_apps, color='grey', linestyle='--', \n",
    "                  linewidth=2, label=f'Total Submitted ({total_submitted_apps})')\n",
    "\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.annotate(f'{int(height)}',\n",
    "                       xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                       xytext=(0, 3), textcoords=\"offset points\",\n",
    "                       ha='center', va='bottom', fontsize=16, fontweight='bold')\n",
    "        \n",
    "\n",
    "        ax.set_ylabel('Number of Executed Apps', fontsize=16, fontweight='bold')\n",
    "        ax.tick_params(axis='x', labelsize=16)\n",
    "        ax.tick_params(axis='y', labelsize=16)\n",
    "        ax.set_ylim(0, total_submitted_apps * 1.2)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.legend(fontsize=14)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        pdf.savefig(fig, bbox_inches='tight', dpi=300)\n",
    "        plt.close(fig)\n",
    "    \n",
    "    return f\"Success analysis chart saved to '{output_filename}'\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    processed_data = process_all_scenarios()\n",
    "    \n",
    "    if not processed_data.empty:\n",
    "        result = create_success_chart(processed_data)\n",
    "        return result\n",
    "    else:\n",
    "        return \"No successful applications found in any scenario.\"\n",
    "\n",
    "# Run the analysis\n",
    "if __name__ == \"__main__\":\n",
    "    result = main()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeb92c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait time analysis chart saved to 'wait_time_analysis.pdf'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "SCENARIOS_TO_PROCESS = {\n",
    "    'Guaranteed': 'data/results_combined_guaranteed_20250815_153205.csv',\n",
    "    'Extreme': 'data/results_combined_extreme_20250815_144056.csv',\n",
    "    'Clairvoyant': 'data/results_combined_clairvoyant_20250818_101541.csv'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def calculate_pending_times(file_path, scenario_name):\n",
    "    \"\"\"Calculate pending times for all applications in a scenario\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce')\n",
    "        df.dropna(subset=['Timestamp', 'Job Name', 'Pod Status'], inplace=True)\n",
    "        \n",
    "        pending_times = []\n",
    "        \n",
    "        for app_name in df['Job Name'].unique():\n",
    "            app_df = df[df['Job Name'] == app_name].sort_values('Timestamp')\n",
    "            pending_entries = app_df[app_df['Pod Status'] == 'Pending']\n",
    "            \n",
    "            if pending_entries.empty:\n",
    "                continue\n",
    "            \n",
    "            # Find first pending time and exit from pending\n",
    "            first_pending_time = pending_entries['Timestamp'].iloc[0]\n",
    "            exit_pending_entry = app_df[app_df['Timestamp'] > first_pending_time]\n",
    "            exit_pending_entry = exit_pending_entry[exit_pending_entry['Pod Status'] != 'Pending']\n",
    "            \n",
    "            if exit_pending_entry.empty:\n",
    "                continue\n",
    "                \n",
    "            exit_pending_time = exit_pending_entry['Timestamp'].iloc[0]\n",
    "            pending_duration_seconds = (exit_pending_time - first_pending_time).total_seconds()\n",
    "            \n",
    "            pending_times.append({\n",
    "                'Job_Name': app_name,\n",
    "                'Pending_Time_sec': pending_duration_seconds\n",
    "            })\n",
    "        \n",
    "        if not pending_times:\n",
    "            return None\n",
    "            \n",
    "        results_df = pd.DataFrame(pending_times)\n",
    "        results_df['Scenario'] = scenario_name\n",
    "        return results_df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "def process_all_scenarios():\n",
    "    \"\"\"Process all scenario files and calculate pending times\"\"\"\n",
    "    all_times = []\n",
    "    \n",
    "    for scenario, file_path in SCENARIOS_TO_PROCESS.items():\n",
    "        result_df = calculate_pending_times(file_path, scenario)\n",
    "        if result_df is not None:\n",
    "            all_times.append(result_df)\n",
    "    \n",
    "    if not all_times:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    return pd.concat(all_times, ignore_index=True)\n",
    "\n",
    "\n",
    "def create_wait_time_chart(df, output_filename='wait_time_analysis.pdf'):\n",
    "    \"\"\"Create chart showing average wait times by scenario\"\"\"\n",
    "    if df.empty:\n",
    "        return \"No data available for visualization\"\n",
    "    \n",
    "    # Calculate average wait times\n",
    "    average_wait_times = df.groupby('Scenario')['Pending_Time_sec'].mean()\n",
    "    scenario_order = ['Guaranteed', 'Clairvoyant', 'Extreme']\n",
    "    average_wait_times = average_wait_times.reindex(scenario_order)\n",
    "\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.rcParams['font.size'] = 16\n",
    "    \n",
    "    with PdfPages(output_filename) as pdf:\n",
    "        fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "        scenario_names = average_wait_times.index.tolist()\n",
    "        avg_times = average_wait_times.values.tolist()\n",
    "        colors = sns.color_palette('Blues', n_colors=len(scenario_names))\n",
    "        \n",
    "        bars = ax.bar(scenario_names, avg_times, color=colors, \n",
    "                     edgecolor='black', width=0.8)\n",
    "        \n",
    "\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.annotate(f'{height:.2f}s',\n",
    "                       xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                       xytext=(0, 3), textcoords=\"offset points\",\n",
    "                       ha='center', va='bottom', fontsize=16, fontweight='bold')\n",
    "\n",
    "        ax.set_ylabel('Average Wait Time (seconds)', fontsize=16, fontweight='bold')\n",
    "        ax.tick_params(axis='x', labelsize=16)\n",
    "        ax.tick_params(axis='y', labelsize=16)\n",
    "        ax.set_ylim(0, max(avg_times) * 1.15)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        pdf.savefig(fig, bbox_inches='tight', dpi=300)\n",
    "        plt.close(fig)\n",
    "    \n",
    "    return f\"Wait time analysis chart saved to '{output_filename}'\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    processed_data = process_all_scenarios()\n",
    "    \n",
    "    if not processed_data.empty:\n",
    "        result = create_wait_time_chart(processed_data)\n",
    "        return result\n",
    "    else:\n",
    "        return \"No pending time data found in any scenario.\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = main()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc00f796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slowdown analysis charts saved to: slowdown_guaranteed.pdf, slowdown_clairvoyant.pdf, slowdown_extreme.pdf\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "ISOLATED_RUN_FILES = [\n",
    "    'data/results_hpc_profiling_20250815_105543.csv',\n",
    "    'data/results_nas_profiling_20250815_000950.csv'\n",
    "]\n",
    "\n",
    "SCENARIO_FILES = {\n",
    "    'Guaranteed': 'data/results_combined_guaranteed_20250815_153205.csv',\n",
    "    'Clairvoyant': 'data/results_combined_clairvoyant_20250818_101541.csv',\n",
    "    'Extreme': 'data/results_combined_extreme_20250815_144056.csv'\n",
    "}\n",
    "\n",
    "CLAIRVOYANT_EP_LOG_FILE = 'data/logs_clairvoyant/run-nas-ep-d-x.log'\n",
    "\n",
    "APP_DISPLAY_NAMES = [\n",
    "    'NAS-BT', 'NAS-CG', 'NAS-EP', 'NAS-MG', 'NAS-SP',\n",
    "    'SIM-pulse_spikes', 'SIM-sawtooth', 'SIM-staircase', 'SIM-epochs' \n",
    "]\n",
    "\n",
    "def get_display_name(original_name):\n",
    "    \"\"\"Standardize job names for consistent filtering\"\"\"\n",
    "    name = original_name.lower()\n",
    "    name_mapping = {\n",
    "        'nas-bt': 'NAS-BT',\n",
    "        'nas-cg': 'NAS-CG', \n",
    "        'nas-ep': 'NAS-EP',\n",
    "        'nas-mg': 'NAS-MG',\n",
    "        'nas-sp': 'NAS-SP',\n",
    "        'hpc-pulse-spikes': 'SIM-pulse_spikes',\n",
    "        'hpc-sawtooth': 'SIM-sawtooth',\n",
    "        'hpc-staged_plateau': 'SIM-staged_plateau',\n",
    "        'hpc-staircase': 'SIM-staircase',\n",
    "        'hpc-epochs': 'SIM-epochs'\n",
    "    }\n",
    "    \n",
    "    for key, value in name_mapping.items():\n",
    "        if key in name:\n",
    "            return value\n",
    "    return original_name\n",
    "\n",
    "def get_execution_times(file_path):\n",
    "    \"\"\"Extract execution times from CSV file\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce')\n",
    "        df.dropna(subset=['Timestamp', 'Job Name', 'Pod Status'], inplace=True)\n",
    "        \n",
    "        metrics = []\n",
    "        for raw_app_name in df['Job Name'].unique():\n",
    "            app_df = df[df['Job Name'] == raw_app_name].sort_values('Timestamp')\n",
    "            running_time = app_df[app_df['Pod Status'] == 'Running']['Timestamp'].min()\n",
    "            succeeded_time = app_df[app_df['Pod Status'] == 'Succeeded']['Timestamp'].min()\n",
    "            \n",
    "            if pd.notna(running_time) and pd.notna(succeeded_time):\n",
    "                execution_time = (succeeded_time - running_time).total_seconds()\n",
    "                metrics.append({\n",
    "                    'Job_Name': get_display_name(raw_app_name),\n",
    "                    'Execution_Time_sec': execution_time,\n",
    "                    'Completion_Time': succeeded_time\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(metrics)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def parse_ep_log_file(file_path):\n",
    "    \"\"\"Parse execution time from EP log file\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if 'Time in seconds =' in line:\n",
    "                    time_str = line.split('=')[1].strip()\n",
    "                    return float(time_str)\n",
    "        return None\n",
    "    except (FileNotFoundError, ValueError, IndexError):\n",
    "        return None\n",
    "\n",
    "def process_isolated_runs():\n",
    "    \"\"\"Process isolated run files to get baseline times\"\"\"\n",
    "    isolated_dfs = [get_execution_times(file) for file in ISOLATED_RUN_FILES]\n",
    "    isolated_df = pd.concat(isolated_dfs, ignore_index=True)\n",
    "    return isolated_df.groupby('Job_Name')['Execution_Time_sec'].mean()\n",
    "\n",
    "def process_scenario_runs():\n",
    "    \"\"\"Process scenario files and calculate slowdown\"\"\"\n",
    "    scenario_times = []\n",
    "    \n",
    "    for scenario, file_path in SCENARIO_FILES.items():\n",
    "        df = get_execution_times(file_path)\n",
    "        if not df.empty:\n",
    "            df['Scenario'] = scenario\n",
    "            scenario_times.append(df)\n",
    "    \n",
    "    if not scenario_times:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    scenario_df = pd.concat(scenario_times, ignore_index=True)\n",
    "    scenario_df = scenario_df.sort_values('Completion_Time')\n",
    "    scenario_df = scenario_df.drop_duplicates(subset=['Scenario', 'Job_Name'], keep='last')\n",
    "\n",
    "    ep_execution_time = parse_ep_log_file(CLAIRVOYANT_EP_LOG_FILE)\n",
    "    if ep_execution_time is not None:\n",
    "        condition = ~((scenario_df['Scenario'] == 'Clairvoyant') & (scenario_df['Job_Name'] == 'NAS-EP'))\n",
    "        scenario_df = scenario_df[condition]\n",
    "        \n",
    "        ep_special_data = pd.DataFrame([{\n",
    "            'Scenario': 'Clairvoyant', \n",
    "            'Job_Name': 'NAS-EP', \n",
    "            'Execution_Time_sec': ep_execution_time,\n",
    "            'Completion_Time': pd.NaT\n",
    "        }])\n",
    "        scenario_df = pd.concat([scenario_df, ep_special_data], ignore_index=True)\n",
    "    \n",
    "    return scenario_df\n",
    "\n",
    "def calculate_slowdown(scenario_df, isolated_times_map):\n",
    "    \"\"\"Calculate slowdown relative to isolated runs\"\"\"\n",
    "    scenario_df['Isolated_Time_sec'] = scenario_df['Job_Name'].map(isolated_times_map)\n",
    "    scenario_df.dropna(subset=['Isolated_Time_sec'], inplace=True)\n",
    "    scenario_df['Slowdown'] = scenario_df.apply(\n",
    "        lambda row: row['Execution_Time_sec'] / row['Isolated_Time_sec'] if row['Isolated_Time_sec'] > 0 else 0,\n",
    "        axis=1\n",
    "    )\n",
    "    return scenario_df\n",
    "\n",
    "\n",
    "def create_slowdown_charts(df):\n",
    "    \"\"\"Create individual slowdown charts for each scenario\"\"\"\n",
    "    if df.empty:\n",
    "        return \"No data available for visualization\"\n",
    "    \n",
    "    scenario_order = ['Guaranteed', 'Clairvoyant', 'Extreme']\n",
    "    colors = sns.color_palette(\"Greens\", n_colors=len(APP_DISPLAY_NAMES))\n",
    "    app_color_map = {app: color for app, color in zip(APP_DISPLAY_NAMES, colors)}\n",
    "    \n",
    "\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.rcParams['font.size'] = 12\n",
    "    \n",
    "    generated_files = []\n",
    "\n",
    "    for scenario in scenario_order:\n",
    "        output_filename = f'slowdown_{scenario.lower()}.pdf'\n",
    "        \n",
    "        with PdfPages(output_filename) as pdf:\n",
    "            fig, ax = plt.subplots(figsize=(14, 8))\n",
    "            \n",
    "            subset_df = df[df['Scenario'] == scenario].copy()\n",
    "            subset_df['Job_Name'] = pd.Categorical(subset_df['Job_Name'], \n",
    "                                                 categories=APP_DISPLAY_NAMES, ordered=True)\n",
    "            subset_df = subset_df.sort_values('Job_Name')\n",
    "            \n",
    "            bar_colors = [app_color_map.get(app, '#CCCCCC') for app in subset_df['Job_Name']]\n",
    "            bars = ax.bar(subset_df['Job_Name'], subset_df['Slowdown'], color=bar_colors)\n",
    "            ax.axhline(y=1, color='black', linestyle='--', linewidth=2, \n",
    "                      label='Isolated Performance')\n",
    "            \n",
    "\n",
    "            max_slowdown = subset_df['Slowdown'].max()\n",
    "            y_max = max(2.0, max_slowdown * 1.1)\n",
    "            ax.set_ylim(0, y_max)\n",
    "            \n",
    "            #ax.set_title(f'{scenario} Scenario - Application Slowdown', fontsize=16, fontweight='bold')\n",
    "            ax.set_ylabel('Slowdown', fontsize=16, fontweight='bold')\n",
    "            ax.tick_params(axis='x', rotation=45, labelsize=16)\n",
    "            ax.tick_params(axis='y', labelsize=16)\n",
    "            \n",
    "\n",
    "            for bar, slowdown in zip(bars, subset_df['Slowdown']):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                       f'{slowdown:.2f}', ha='center', va='bottom', \n",
    "                       fontsize=16, fontweight='bold')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            pdf.savefig(fig, bbox_inches='tight', dpi=300)\n",
    "            plt.close(fig)\n",
    "        \n",
    "        generated_files.append(output_filename)\n",
    "    \n",
    "    files_list = ', '.join(generated_files)\n",
    "    return f\"Slowdown analysis charts saved to: {files_list}\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    isolated_times_map = process_isolated_runs()\n",
    "    scenario_df = process_scenario_runs()\n",
    "    \n",
    "    if scenario_df.empty or isolated_times_map.empty:\n",
    "        return \"No data available for slowdown analysis\"\n",
    "    \n",
    "    final_df = calculate_slowdown(scenario_df, isolated_times_map)\n",
    "    \n",
    "    if not final_df.empty:\n",
    "        result = create_slowdown_charts(final_df)\n",
    "        return result\n",
    "    else:\n",
    "        return \"No valid slowdown data could be calculated\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = main()\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "awsbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
